{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003ca0ee",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Step-by-Step Tutorial\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step explanation of how Retrieval-Augmented Generation works in our All-About-RAG system.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines the power of large language models with external knowledge retrieval. Instead of relying only on the model's training data, RAG:\n",
    "\n",
    "1. **Retrieves** relevant information from a knowledge base\n",
    "2. **Augments** the query with this context\n",
    "3. **Generates** accurate, grounded responses\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "Our RAG system follows this pipeline:\n",
    "1. **Document Loading** - Load documents from various formats\n",
    "2. **Text Chunking** - Split documents into manageable pieces\n",
    "3. **Embedding Generation** - Convert text to numerical vectors\n",
    "4. **Vector Storage** - Store vectors in a searchable database\n",
    "5. **Query Processing** - Handle user questions with retrieval + generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0964c0f",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b42a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All RAG components imported successfully!\n",
      "ğŸ“š Ready to explore the RAG pipeline\n"
     ]
    }
   ],
   "source": [
    "# Import all RAG components\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add project root to path\n",
    "\n",
    "from src.rag.data_loader import load_all_documents\n",
    "from src.rag.chunking import ChunkingPipeline\n",
    "from src.rag.embedding import EmbeddingPipeline\n",
    "from src.rag.vectorstore import FaissVectorStore\n",
    "from src.rag.search import RAGSearch\n",
    "\n",
    "# Additional imports for demonstration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All RAG components imported successfully!\")\n",
    "print(\"ğŸ“š Ready to explore the RAG pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58942d53",
   "metadata": {},
   "source": [
    "## Step 2: Document Loading\n",
    "\n",
    "The first step is loading documents from various formats (PDF, TXT, CSV, Excel, Word, JSON).\n",
    "\n",
    "**What happens here:**\n",
    "- Recursively scan the data directory\n",
    "- Use specialized loaders for each file type\n",
    "- Convert files into LangChain Document objects\n",
    "- Handle errors gracefully for corrupted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde9720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Loading documents from data/ directory...\n",
      "[DEBUG] Data path: D:\\GITHUB\\All-About-RAG\\data\n",
      "[DEBUG] Supported formats: ['.pdf', '.txt', '.csv', '.xlsx', '.docx', '.json']\n",
      "[DEBUG] Found 2 PDF files: ['D:\\\\GITHUB\\\\All-About-RAG\\\\data\\\\Neeraj_Tiwari_CV_Oct25 (1).pdf', 'D:\\\\GITHUB\\\\All-About-RAG\\\\data\\\\The Ultimate Python Cheat Sheet.pdf']\n",
      "[DEBUG] Loading PDF: D:\\GITHUB\\All-About-RAG\\data\\Neeraj_Tiwari_CV_Oct25 (1).pdf\n",
      "[DEBUG] Loaded 2 PDF docs from D:\\GITHUB\\All-About-RAG\\data\\Neeraj_Tiwari_CV_Oct25 (1).pdf\n",
      "[DEBUG] Loading PDF: D:\\GITHUB\\All-About-RAG\\data\\The Ultimate Python Cheat Sheet.pdf\n",
      "[DEBUG] Loaded 1 PDF docs from D:\\GITHUB\\All-About-RAG\\data\\The Ultimate Python Cheat Sheet.pdf\n",
      "[DEBUG] Found 0 TXT files: []\n",
      "[DEBUG] Found 0 CSV files: []\n",
      "[DEBUG] Found 0 Excel files: []\n",
      "[DEBUG] Found 0 Word files: []\n",
      "[DEBUG] Found 0 JSON files: []\n",
      "[DEBUG] Total loaded documents: 3\n",
      "âœ… Loaded 3 document objects\n",
      "ğŸ“„ Document types found: ['Document', 'Document', 'Document']\n",
      "\n",
      "ğŸ“– Sample document content (first 200 chars):\n",
      "'Neeraj Tiwari \n",
      "             Jubilee Green, Papworth Everard, Cambridge-CB233RZ| neeraztiwari@gmail.com | +44 7881387635 \n",
      " \n",
      "Accomplished AI and ML Specialist with 6+ years of experience, currently inno...'\n",
      "ğŸ“Š Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-08T03:41:06-07:00', 'author': 'Ankita', 'moddate': '2025-10-08T03:41:06-07:00', 'source': 'D:\\\\GITHUB\\\\All-About-RAG\\\\data\\\\Neeraj_Tiwari_CV_Oct25 (1).pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the data directory\n",
    "print(\"ğŸ” Loading documents from data/ directory...\")\n",
    "\n",
    "docs = load_all_documents(\"../data\")  # Use relative path from notebooks folder\n",
    "\n",
    "print(f\"âœ… Loaded {len(docs)} document objects\")\n",
    "print(f\"ğŸ“„ Document types found: {[type(doc).__name__ for doc in docs[:3]]}\")\n",
    "\n",
    "# Show a sample document\n",
    "if docs:\n",
    "    print(f\"\\nğŸ“– Sample document content (first 200 chars):\")\n",
    "    print(f\"'{docs[0].page_content[:200]}...'\")\n",
    "    print(f\"ğŸ“Š Metadata: {docs[0].metadata}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No documents found. Please ensure you have files in the data/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43e684",
   "metadata": {},
   "source": [
    "## Step 3: Text Chunking\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective processing.\n",
    "\n",
    "**Why chunking?**\n",
    "- LLMs have token limits\n",
    "- Better retrieval precision\n",
    "- Maintains context with overlap\n",
    "\n",
    "**Our approach:**\n",
    "- Chunk size: 1000 characters\n",
    "- Overlap: 200 characters\n",
    "- Recursive splitting on natural boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbe0435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Initializing text chunking pipeline...\n",
      "[INFO] Chunking config - Size: 1000, Overlap: 200\n",
      "âœ… Chunking pipeline ready!\n",
      "ğŸ“ Chunk size: 1000 characters\n",
      "ğŸ”„ Overlap: 200 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chunking pipeline\n",
    "print(\"âœ‚ï¸ Initializing text chunking pipeline...\")\n",
    "\n",
    "chunk_pipeline = ChunkingPipeline(\n",
    "    chunk_size=1000,      # Maximum characters per chunk\n",
    "    chunk_overlap=200     # Overlap between chunks for context\n",
    ")\n",
    "\n",
    "print(\"âœ… Chunking pipeline ready!\")\n",
    "print(f\"ğŸ“ Chunk size: {chunk_pipeline.chunk_size} characters\")\n",
    "print(f\"ğŸ”„ Overlap: {chunk_pipeline.chunk_overlap} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0c790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Splitting documents into chunks...\n",
      "[INFO] Split 3 documents into 22 chunks.\n",
      "âœ… Created 22 chunks from 3 documents\n",
      "ğŸ“Š Average chunk length: 909.6 characters\n",
      "ğŸ“ˆ Min/Max length: 301 / 996 characters\n",
      "\n",
      "ğŸ“„ Sample chunk (first 150 chars):\n",
      "'Neeraj Tiwari \n",
      "             Jubilee Green, Papworth Everard, Cambridge-CB233RZ| neeraztiwari@gmail.com | +44 7881387635 \n",
      " \n",
      "Accomplished AI and ML Spec...'\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "print(\"ğŸ”„ Splitting documents into chunks...\")\n",
    "\n",
    "chunks = chunk_pipeline.chunk_documents(docs)\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {len(docs)} documents\")\n",
    "\n",
    "# Analyze chunk statistics\n",
    "if chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "\n",
    "    print(f\"ğŸ“Š Average chunk length: {avg_length:.1f} characters\")\n",
    "    print(f\"ğŸ“ˆ Min/Max length: {min(chunk_lengths)} / {max(chunk_lengths)} characters\")\n",
    "\n",
    "    # Show a sample chunk\n",
    "    print(f\"\\nğŸ“„ Sample chunk (first 150 chars):\")\n",
    "    print(f\"'{chunks[0].page_content[:150]}...'\")\n",
    "else:\n",
    "    print(\"âš ï¸ No chunks created. Please ensure documents were loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44180aab",
   "metadata": {},
   "source": [
    "## Step 4: Embedding Generation\n",
    "\n",
    "Convert text chunks into numerical vectors that capture semantic meaning.\n",
    "\n",
    "**How embeddings work:**\n",
    "- Use SentenceTransformer model (all-MiniLM-L6-v2)\n",
    "- 384-dimensional vectors\n",
    "- Capture semantic similarity\n",
    "- Enable mathematical comparison of text\n",
    "\n",
    "**Batch processing:**\n",
    "- Process multiple chunks efficiently\n",
    "- GPU acceleration when available\n",
    "- Progress tracking for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e8fd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Initializing embedding pipeline...\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Chunking config - Size: 1000, Overlap: 200\n",
      "âœ… Embedding model loaded!\n",
      "ğŸ¤– Model: 384D vectors\n",
      "âš¡ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding pipeline\n",
    "print(\"ğŸ§  Initializing embedding pipeline...\")\n",
    "\n",
    "embed_pipeline = EmbeddingPipeline(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  # Fast, efficient model\n",
    ")\n",
    "\n",
    "print(\"âœ… Embedding model loaded!\")\n",
    "print(f\"ğŸ¤– Model: {embed_pipeline.model.get_sentence_embedding_dimension()}D vectors\")\n",
    "print(f\"âš¡ Device: {embed_pipeline.model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026a3398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Generating embeddings for chunks...\n",
      "(This may take a moment for large document collections)\n",
      "[INFO] Generating embeddings for 22 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings shape: (22, 384)\n",
      "âœ… Embeddings generated!\n",
      "ğŸ“Š Embedding matrix shape: (22, 384)\n",
      "ğŸ”¢ Total vectors: 22\n",
      "ğŸ“ Vector dimensions: 384\n",
      "\n",
      "ğŸ§® Sample embedding (first 5 dimensions): [-0.05511471 -0.08162459 -0.01212582 -0.05076062  0.03955764]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(\"ğŸ”¢ Generating embeddings for chunks...\")\n",
    "print(\"(This may take a moment for large document collections)\")\n",
    "\n",
    "embeddings = embed_pipeline.embed_chunks(chunks)\n",
    "\n",
    "print(\"âœ… Embeddings generated!\")\n",
    "print(f\"ğŸ“Š Embedding matrix shape: {embeddings.shape}\")\n",
    "print(f\"ğŸ”¢ Total vectors: {embeddings.shape[0]}\")\n",
    "print(f\"ğŸ“ Vector dimensions: {embeddings.shape[1]}\")\n",
    "\n",
    "# Show a sample embedding\n",
    "if len(embeddings) > 0:\n",
    "    print(f\"\\nğŸ§® Sample embedding (first 5 dimensions): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4001b7b",
   "metadata": {},
   "source": [
    "## Step 5: Vector Storage\n",
    "\n",
    "Store embeddings in a vector database for fast similarity search.\n",
    "\n",
    "**FAISS Vector Store:**\n",
    "- Facebook AI Similarity Search library\n",
    "- Optimized for L2 (Euclidean) distance\n",
    "- Fast retrieval of similar vectors\n",
    "- Persistent storage with metadata\n",
    "\n",
    "**Why FAISS?**\n",
    "- CPU-based (no GPU required)\n",
    "- Scales to millions of vectors\n",
    "- Industry standard for vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3cdb0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Initializing FAISS vector store...\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Vector store directory: faiss_store\n",
      "âœ… Vector store ready!\n",
      "ğŸ“ Storage directory: faiss_store\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "print(\"ğŸ’¾ Initializing FAISS vector store...\")\n",
    "\n",
    "vector_store = FaissVectorStore(\n",
    "    persist_dir=\"faiss_store\",\n",
    "    embedding_model=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store ready!\")\n",
    "print(f\"ğŸ“ Storage directory: {vector_store.persist_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a9d0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Adding embeddings to vector store...\n",
      "[INFO] Added 22 vectors to Faiss index.\n",
      "[INFO] Saved Faiss index and metadata to faiss_store\n",
      "âœ… Vectors stored and saved!\n",
      "ğŸ—„ï¸ Total vectors in store: 22\n",
      "ğŸ’¾ Index saved to: faiss_store/\n"
     ]
    }
   ],
   "source": [
    "# Add embeddings to the vector store\n",
    "print(\"ğŸ’¾ Adding embeddings to vector store...\")\n",
    "\n",
    "# Prepare metadata for each chunk\n",
    "metadata = [{\"text\": chunk.page_content, \"source\": chunk.metadata.get(\"source\", \"unknown\")} \n",
    "           for chunk in chunks]\n",
    "\n",
    "vector_store.add_embeddings(embeddings, metadata)\n",
    "vector_store.save()\n",
    "\n",
    "print(\"âœ… Vectors stored and saved!\")\n",
    "print(f\"ğŸ—„ï¸ Total vectors in store: {vector_store.index.ntotal}\")\n",
    "print(f\"ğŸ’¾ Index saved to: {vector_store.persist_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cc215",
   "metadata": {},
   "source": [
    "## Step 6: Query Processing\n",
    "\n",
    "Now let's see how the system processes a user query end-to-end.\n",
    "\n",
    "**Query Processing Steps:**\n",
    "1. **Embed the query** - Convert question to vector\n",
    "2. **Similarity search** - Find most relevant chunks\n",
    "3. **Context assembly** - Combine retrieved chunks\n",
    "4. **Response generation** - Use LLM with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52262bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Processing query:\n",
      "'How many years of experience does the candidate have?'\n",
      "\n",
      "ğŸ” Step 1: Embedding the query\n",
      "âœ… Query embedded (shape: (1, 384))\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How many years of experience does the candidate have?\"\n",
    "\n",
    "print(\"â“ Processing query:\")\n",
    "print(f\"'{query}'\")\n",
    "print()\n",
    "\n",
    "# Step 6a: Embed the query\n",
    "print(\"ğŸ” Step 1: Embedding the query\")\n",
    "query_embedding = embed_pipeline.model.encode([query])\n",
    "print(f\"âœ… Query embedded (shape: {query_embedding.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43caab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 2: Finding similar documents\n",
      "âœ… Found 3 most relevant chunks\n",
      "\n",
      "ğŸ“„ Result 1 (relevance: N/A):\n",
      "'HPC Server, Camera, Lidar, Thermal Camera, Accelerometer, Gyroscope, FTP Camera, FTP Server. \n",
      "â€¢ Spec...'\n",
      "\n",
      "ğŸ“„ Result 2 (relevance: N/A):\n",
      "'â€¢ Developed state-of-the-art DL algorithms in python, focusing on enhancing visual accuracy and deta...'\n",
      "\n",
      "ğŸ“„ Result 3 (relevance: N/A):\n",
      "'Neeraj Tiwari \n",
      "             Jubilee Green, Papworth Everard, Cambridge-CB233RZ| neeraztiwari@gmail.c...'\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Similarity search\n",
    "print(\"ğŸ” Step 2: Finding similar documents\")\n",
    "\n",
    "top_k = 3  # Number of results to retrieve\n",
    "results = vector_store.search(query_embedding, top_k=top_k)\n",
    "\n",
    "print(f\"âœ… Found {len(results)} most relevant chunks\")\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results, 1):\n",
    "    score = result.get('score', 'N/A')\n",
    "    text_preview = result['metadata']['text'][:100] + \"...\"\n",
    "    print(f\"\\nğŸ“„ Result {i} (relevance: {score}):\")\n",
    "    print(f\"'{text_preview}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f57c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 3: Assembling context\n",
      "âœ… Context assembled!\n",
      "ğŸ“Š Context length: 2681 characters\n",
      "ğŸ“„ Number of chunks combined: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Context assembly\n",
    "print(\"ğŸ” Step 3: Assembling context\")\n",
    "\n",
    "retrieved_texts = [r[\"metadata\"][\"text\"] for r in results]\n",
    "context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "print(\"âœ… Context assembled!\")\n",
    "print(f\"ğŸ“Š Context length: {len(context)} characters\")\n",
    "print(f\"ğŸ“„ Number of chunks combined: {len(retrieved_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ff494c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Step 4: Generating response with LLM\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Vector store directory: faiss_store\n",
      "[INFO] Loaded Faiss index and metadata from faiss_store\n",
      "[INFO] Gemini LLM initialized: gemini-2.5-flash\n",
      "[INFO] LLM config - Temperature: 0.3, Max tokens: 1000\n",
      "[INFO] Querying vector store for: 'How many years of experience does the candidate have?' (top_k=3)\n",
      "âœ… Response generated!\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ FINAL ANSWER:\n",
      "==================================================\n",
      "The candidate has 6+ years of experience.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Response generation\n",
    "print(\"ğŸ¤– Step 4: Generating response with LLM\")\n",
    "\n",
    "# Initialize RAG search system\n",
    "rag_search = RAGSearch()\n",
    "\n",
    "# Generate the final answer\n",
    "response = rag_search.search_and_summarize(query, top_k=top_k)\n",
    "\n",
    "print(\"âœ… Response generated!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ¯ FINAL ANSWER:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b9534",
   "metadata": {},
   "source": [
    "## Step 7: Complete RAG Pipeline Demo\n",
    "\n",
    "Let's put it all together in a single function that demonstrates the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eac31ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Complete RAG Pipeline Demo\n",
      "==================================================\n",
      "1ï¸âƒ£ Loading documents...\n",
      "[DEBUG] Data path: D:\\GITHUB\\All-About-RAG\\data\n",
      "[DEBUG] Supported formats: ['.pdf', '.txt', '.csv', '.xlsx', '.docx', '.json']\n",
      "[DEBUG] Found 2 PDF files: ['D:\\\\GITHUB\\\\All-About-RAG\\\\data\\\\Neeraj_Tiwari_CV_Oct25 (1).pdf', 'D:\\\\GITHUB\\\\All-About-RAG\\\\data\\\\The Ultimate Python Cheat Sheet.pdf']\n",
      "[DEBUG] Loading PDF: D:\\GITHUB\\All-About-RAG\\data\\Neeraj_Tiwari_CV_Oct25 (1).pdf\n",
      "[DEBUG] Loaded 2 PDF docs from D:\\GITHUB\\All-About-RAG\\data\\Neeraj_Tiwari_CV_Oct25 (1).pdf\n",
      "[DEBUG] Loading PDF: D:\\GITHUB\\All-About-RAG\\data\\The Ultimate Python Cheat Sheet.pdf\n",
      "[DEBUG] Loaded 1 PDF docs from D:\\GITHUB\\All-About-RAG\\data\\The Ultimate Python Cheat Sheet.pdf\n",
      "[DEBUG] Found 0 TXT files: []\n",
      "[DEBUG] Found 0 CSV files: []\n",
      "[DEBUG] Found 0 Excel files: []\n",
      "[DEBUG] Found 0 Word files: []\n",
      "[DEBUG] Found 0 JSON files: []\n",
      "[DEBUG] Total loaded documents: 3\n",
      "   âœ… Loaded 3 documents\n",
      "2ï¸âƒ£ Chunking documents...\n",
      "[INFO] Chunking config - Size: 1000, Overlap: 200\n",
      "[INFO] Split 3 documents into 22 chunks.\n",
      "   âœ… Created 22 chunks\n",
      "3ï¸âƒ£ Generating embeddings...\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Chunking config - Size: 1000, Overlap: 200\n",
      "[INFO] Generating embeddings for 22 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings shape: (22, 384)\n",
      "   âœ… Generated 22 embeddings\n",
      "4ï¸âƒ£ Storing in vector database...\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Vector store directory: faiss_store\n",
      "[INFO] Added 22 vectors to Faiss index.\n",
      "[INFO] Saved Faiss index and metadata to faiss_store\n",
      "   âœ… Stored 22 vectors\n",
      "5ï¸âƒ£ Processing query...\n",
      "   Query: 'Summarize the key points from the documents'\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Auto-detected device: cpu\n",
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Vector store directory: faiss_store\n",
      "[INFO] Loaded Faiss index and metadata from faiss_store\n",
      "[INFO] Gemini LLM initialized: gemini-2.5-flash\n",
      "[INFO] LLM config - Temperature: 0.3, Max tokens: 1000\n",
      "[INFO] Querying vector store for: 'Summarize the key points from the documents' (top_k=3)\n",
      "   âœ… Answer generated!\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ ANSWER:\n",
      "==================================================\n",
      "The documents provide information on two main areas:\n",
      "\n",
      "1.  **Python Programming Concepts:**\n",
      "    *   **String Operations:** Covers indexing, slicing, and various string methods like `strip()`, `lower()`, `upper()`, `startswith()`, `endswith()`, `find()`, `replace()`, `join()`, `len()`, and the `in` operator for membership.\n",
      "    *   **Dictionary Operations:** Explains how to define, read, write, and iterate through dictionaries. It also details accessing keys (`keys()`) and values (`values()`) and using the `in` operator for membership checks.\n",
      "    *   **List & Set Comprehension:** Describes these as concise Python ways to create lists and sets.\n",
      "    *   **Membership Operator:** Highlights the `in` keyword for checking element presence in sets, lists, or dictionaries, noting that set membership is faster.\n",
      "\n",
      "2.  **Individual's Professional Profile/Expertise:**\n",
      "    *   **Hardware/Tools:** HPC Server, Camera, Lidar, Thermal Camera, Accelerometer, Gyroscope, FTP Camera, FTP Server.\n",
      "    *   **Specialized Skills:** Hardware Acceleration, Computer Vision, Machine Learning, Deep Learning, Data Analysis.\n",
      "    *   **Other Expertise:** Agile/Scrum, Microsoft 365, Patents & Research.\n",
      "    *   **Research & Development:** Includes a paper on \"Out-of-distribution classification\" (2022) and a filed patent (2021) for a \"System and method for estimating the depth of at least one at least partially water-filled pothole.\"\n",
      "    *   **Courses & Learning:** Specialization in Self-Driving Car (University of Toronto, 2021) and Neural Networks and Deep Learning (2018).\n",
      "    *   **Languages:** Proficient in English and Hindi (reading, writing, speaking).\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The documents provide information on two main areas:\\n\\n1.  **Python Programming Concepts:**\\n    *   **String Operations:** Covers indexing, slicing, and various string methods like `strip()`, `lower()`, `upper()`, `startswith()`, `endswith()`, `find()`, `replace()`, `join()`, `len()`, and the `in` operator for membership.\\n    *   **Dictionary Operations:** Explains how to define, read, write, and iterate through dictionaries. It also details accessing keys (`keys()`) and values (`values()`) and using the `in` operator for membership checks.\\n    *   **List & Set Comprehension:** Describes these as concise Python ways to create lists and sets.\\n    *   **Membership Operator:** Highlights the `in` keyword for checking element presence in sets, lists, or dictionaries, noting that set membership is faster.\\n\\n2.  **Individual\\'s Professional Profile/Expertise:**\\n    *   **Hardware/Tools:** HPC Server, Camera, Lidar, Thermal Camera, Accelerometer, Gyroscope, FTP Camera, FTP Server.\\n    *   **Specialized Skills:** Hardware Acceleration, Computer Vision, Machine Learning, Deep Learning, Data Analysis.\\n    *   **Other Expertise:** Agile/Scrum, Microsoft 365, Patents & Research.\\n    *   **Research & Development:** Includes a paper on \"Out-of-distribution classification\" (2022) and a filed patent (2021) for a \"System and method for estimating the depth of at least one at least partially water-filled pothole.\"\\n    *   **Courses & Learning:** Specialization in Self-Driving Car (University of Toronto, 2021) and Neural Networks and Deep Learning (2018).\\n    *   **Languages:** Proficient in English and Hindi (reading, writing, speaking).'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete_rag_demo(query=\"What is this document about?\"):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline demonstration in one function.\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Starting Complete RAG Pipeline Demo\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Load documents\n",
    "    print(\"1ï¸âƒ£ Loading documents...\")\n",
    "    docs = load_all_documents(\"../data\")  # Use relative path from notebooks folder\n",
    "    print(f\"   âœ… Loaded {len(docs)} documents\")\n",
    "    \n",
    "    # 2. Chunk documents\n",
    "    print(\"2ï¸âƒ£ Chunking documents...\")\n",
    "    chunker = ChunkingPipeline()\n",
    "    chunks = chunker.chunk_documents(docs)\n",
    "    print(f\"   âœ… Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # 3. Generate embeddings\n",
    "    print(\"3ï¸âƒ£ Generating embeddings...\")\n",
    "    embedder = EmbeddingPipeline()\n",
    "    embeddings = embedder.embed_chunks(chunks)\n",
    "    print(f\"   âœ… Generated {embeddings.shape[0]} embeddings\")\n",
    "    \n",
    "    # 4. Store in vector database\n",
    "    print(\"4ï¸âƒ£ Storing in vector database...\")\n",
    "    store = FaissVectorStore(\"faiss_store\")\n",
    "    metadata = [{\"text\": chunk.page_content} for chunk in chunks]\n",
    "    store.add_embeddings(embeddings, metadata)\n",
    "    store.save()\n",
    "    print(f\"   âœ… Stored {store.index.ntotal} vectors\")\n",
    "    \n",
    "    # 5. Process query\n",
    "    print(\"5ï¸âƒ£ Processing query...\")\n",
    "    print(f\"   Query: '{query}'\")\n",
    "    \n",
    "    # Embed query\n",
    "    query_vec = embedder.model.encode([query])\n",
    "    \n",
    "    # Search\n",
    "    results = store.search(query_vec, top_k=3)\n",
    "    \n",
    "    # Generate response\n",
    "    rag = RAGSearch()\n",
    "    answer = rag.search_and_summarize(query, top_k=3)\n",
    "    \n",
    "    print(\"   âœ… Answer generated!\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ¯ ANSWER:\")\n",
    "    print(\"=\"*50)\n",
    "    print(answer)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Run the demo\n",
    "complete_rag_demo(\"Summarize the key points from the documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc099c",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "ğŸ¯ **RAG combines retrieval and generation:**\n",
    "- Retrieval finds relevant information\n",
    "- Generation creates coherent responses\n",
    "- Result: Accurate, up-to-date answers\n",
    "\n",
    "ğŸ› ï¸ **Our implementation uses:**\n",
    "- **LangChain** for document loading\n",
    "- **SentenceTransformers** for embeddings\n",
    "- **FAISS** for vector search\n",
    "- **Google Gemini** for generation\n",
    "\n",
    "ğŸ“Š **Performance characteristics:**\n",
    "- Fast similarity search (FAISS)\n",
    "- Semantic understanding (embeddings)\n",
    "- Contextual responses (LLM with retrieved docs)\n",
    "\n",
    "ğŸ”„ **The pipeline is modular:**\n",
    "- Each step can be optimized independently\n",
    "- Easy to experiment with different models\n",
    "- Scalable to large document collections\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different embedding models\n",
    "- Experiment with chunk sizes and overlap\n",
    "- Add more document formats\n",
    "- Implement advanced retrieval techniques\n",
    "- Fine-tune the LLM prompts\n",
    "\n",
    "Happy RAG experimenting! ğŸ¤–ğŸ“š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
